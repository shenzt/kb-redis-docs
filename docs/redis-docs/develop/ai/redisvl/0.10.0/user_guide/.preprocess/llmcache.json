{
  "schema_version": 1,
  "content_hash": "sha1:b1d75ff3487688bd40a628b07b73be7311f3b364",
  "generated_at": "2026-02-23T23:22:19Z",
  "model": "deepseek-chat",
  "prompt_version": "doc_preprocess_v1",
  "llm_status": "ok",
  "contextual_summary": "The document covers using RedisVL's SemanticCache to cache LLM responses based on semantic similarity to reduce API costs and latency, including initialization, operations, customization, and access controls.",
  "doc_type": "tutorial",
  "quality_score": 8,
  "key_concepts": [
    "SemanticCache",
    "RedisVL",
    "TTL policies",
    "vectorizer",
    "access controls"
  ],
  "gap_flags": [
    "missing_command",
    "missing_config"
  ],
  "evidence_flags": {
    "has_command": false,
    "has_config": true,
    "has_code_block": true,
    "has_steps": false
  }
}